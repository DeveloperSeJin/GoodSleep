{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== [Fold-0] ==========\n",
      "\n",
      "Load training set:\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4022E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4042E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4162E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4092E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4561F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4481F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4012E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4672G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4292G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4212E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4331F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4821G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4671G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4702E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4822G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4761E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4021E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4312E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4032E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4801G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4181E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4302E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4182E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4651E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4502E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4081E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4731E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4622E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4251E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4281G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4082E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4511E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4461F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4271F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4002E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4752E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4141E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4611E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4282G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4031E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4542F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4652E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4412E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4811G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4471F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4551F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4321E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4572F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4382F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4252E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4072E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4351F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4371F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4602E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4501E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4562F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4342F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4571F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4762E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4422E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4301E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4402E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4051E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4142E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4171E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4101E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4491G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4802G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4011E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4052E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4592G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4541F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4582G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4741E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4712E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4352F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4231E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4041E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4601E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4311E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4431E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4732E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4411E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4642E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4242E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4442E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4662E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4131E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4591G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4401E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4211E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4441E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4161E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4071E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4291G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4111E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4062E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4122E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4432E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4121E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4812G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4721E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4751E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4451F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4772G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4151E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4261F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4612E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4091E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4202E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4152E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4332F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4532E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4362F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4661E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4262F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4061E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4531E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4341F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4192E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4462F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4112E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4272F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4621E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4102E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4482F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4711E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4742E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4722E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4322E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4512E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4522E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4452F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4492G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4372F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4191E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4221E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4222E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4381F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4172E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4472F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4641E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4241E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4421E0.npz ...\n",
      " \n",
      "Load Test set:\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4771G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4232E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4632E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4631E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4581G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4701E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4552F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4201E0.npz ...\n",
      " \n",
      "Training set: n_subjects=144\n",
      "(1009, 3000)\n",
      "(1200, 3000)\n",
      "(1003, 3000)\n",
      "(1105, 3000)\n",
      "(1237, 3000)\n",
      "(2027, 3000)\n",
      "(1186, 3000)\n",
      "(1021, 3000)\n",
      "(1605, 3000)\n",
      "(808, 3000)\n",
      "(1888, 3000)\n",
      "(1704, 3000)\n",
      "(1968, 3000)\n",
      "(1515, 3000)\n",
      "(1366, 3000)\n",
      "(1683, 3000)\n",
      "(1025, 3000)\n",
      "(1181, 3000)\n",
      "(911, 3000)\n",
      "(1241, 3000)\n",
      "(964, 3000)\n",
      "(854, 3000)\n",
      "(920, 3000)\n",
      "(2644, 3000)\n",
      "(1103, 3000)\n",
      "(1134, 3000)\n",
      "(2667, 3000)\n",
      "(1823, 3000)\n",
      "(972, 3000)\n",
      "(1127, 3000)\n",
      "(1054, 3000)\n",
      "(1087, 3000)\n",
      "(983, 3000)\n",
      "(1052, 3000)\n",
      "(1127, 3000)\n",
      "(1049, 3000)\n",
      "(1004, 3000)\n",
      "(1652, 3000)\n",
      "(1070, 3000)\n",
      "(952, 3000)\n",
      "(1148, 3000)\n",
      "(1929, 3000)\n",
      "(924, 3000)\n",
      "(1293, 3000)\n",
      "(1187, 3000)\n",
      "(1047, 3000)\n",
      "(1560, 3000)\n",
      "(1095, 3000)\n",
      "(1871, 3000)\n",
      "(1020, 3000)\n",
      "(1273, 3000)\n",
      "(976, 3000)\n",
      "(918, 3000)\n",
      "(2043, 3000)\n",
      "(1326, 3000)\n",
      "(1148, 3000)\n",
      "(1582, 3000)\n",
      "(1236, 3000)\n",
      "(2662, 3000)\n",
      "(884, 3000)\n",
      "(929, 3000)\n",
      "(1072, 3000)\n",
      "(672, 3000)\n",
      "(952, 3000)\n",
      "(1002, 3000)\n",
      "(1104, 3000)\n",
      "(1101, 3000)\n",
      "(1229, 3000)\n",
      "(1103, 3000)\n",
      "(1246, 3000)\n",
      "(1231, 3000)\n",
      "(1716, 3000)\n",
      "(1175, 3000)\n",
      "(2210, 3000)\n",
      "(1241, 3000)\n",
      "(963, 3000)\n",
      "(904, 3000)\n",
      "(1235, 3000)\n",
      "(1349, 3000)\n",
      "(1054, 3000)\n",
      "(699, 3000)\n",
      "(2318, 3000)\n",
      "(1078, 3000)\n",
      "(2049, 3000)\n",
      "(1775, 3000)\n",
      "(1092, 3000)\n",
      "(1994, 3000)\n",
      "(1028, 3000)\n",
      "(1840, 3000)\n",
      "(1064, 3000)\n",
      "(1578, 3000)\n",
      "(1195, 3000)\n",
      "(1144, 3000)\n",
      "(976, 3000)\n",
      "(1131, 3000)\n",
      "(928, 3000)\n",
      "(1016, 3000)\n",
      "(977, 3000)\n",
      "(962, 3000)\n",
      "(1052, 3000)\n",
      "(1183, 3000)\n",
      "(1031, 3000)\n",
      "(2044, 3000)\n",
      "(1208, 3000)\n",
      "(1324, 3000)\n",
      "(952, 3000)\n",
      "(1597, 3000)\n",
      "(1062, 3000)\n",
      "(1132, 3000)\n",
      "(1021, 3000)\n",
      "(1762, 3000)\n",
      "(1312, 3000)\n",
      "(1056, 3000)\n",
      "(824, 3000)\n",
      "(2026, 3000)\n",
      "(980, 3000)\n",
      "(843, 3000)\n",
      "(1096, 3000)\n",
      "(1501, 3000)\n",
      "(1274, 3000)\n",
      "(1022, 3000)\n",
      "(802, 3000)\n",
      "(1090, 3000)\n",
      "(1445, 3000)\n",
      "(1092, 3000)\n",
      "(1910, 3000)\n",
      "(1413, 3000)\n",
      "(1063, 3000)\n",
      "(1130, 3000)\n",
      "(1021, 3000)\n",
      "(954, 3000)\n",
      "(997, 3000)\n",
      "(1166, 3000)\n",
      "(1040, 3000)\n",
      "(1509, 3000)\n",
      "(1535, 3000)\n",
      "(1099, 3000)\n",
      "(1108, 3000)\n",
      "(1776, 3000)\n",
      "(1773, 3000)\n",
      "(2161, 3000)\n",
      "(1271, 3000)\n",
      "(1673, 3000)\n",
      "(785, 3000)\n",
      "Number of examples = 184490\n",
      "W: 62402\n",
      "N1: 19997\n",
      "N2: 64959\n",
      "N3: 12696\n",
      "REM: 24436\n",
      " \n",
      "Test set: n_subjects = 8\n",
      "(1325, 3000)\n",
      "(1729, 3000)\n",
      "(1107, 3000)\n",
      "(1063, 3000)\n",
      "(1095, 3000)\n",
      "(1717, 3000)\n",
      "(1090, 3000)\n",
      "(1022, 3000)\n",
      "Number of examples = 10148\n",
      "W: 3361\n",
      "N1: 1467\n",
      "N2: 3923\n",
      "N3: 123\n",
      "REM: 1274\n",
      " \n",
      "(18449, 10, 3000)\n",
      "(184490, 3000)\n"
     ]
    }
   ],
   "source": [
    "from dataloader import SeqDataLoader\n",
    "import numpy as np\n",
    "\n",
    "num_folds = 20\n",
    "data_dir = 'data/data2013/eeg_fpz_cz'\n",
    "classes = ['W', 'N1', 'N2', 'N3', 'REM']\n",
    "checkpoint_dir = 'checkpoints-seq2seq-sleep-EDF'\n",
    "output_dir = 'outputs_2013/outputs_eeg_fpz_cz'\n",
    "NUM_CLASS= 7\n",
    "fold_idx = 0\n",
    "\n",
    "kargs = {\n",
    "    'epochs': 120,\n",
    "    'batch_size': 20,\n",
    "    'num_units': 128,\n",
    "    'embed_size': 10,\n",
    "    'input_depth': 3000,\n",
    "    'n_channels': 100,\n",
    "    'bidirectional': False,\n",
    "    'use_attention': True,\n",
    "    'lstm_layers': 2,\n",
    "    'attention_size': 64,\n",
    "    'beam_width': 4,\n",
    "    'use_beamsearch_decode': False,\n",
    "    'max_time_step': 10,\n",
    "    'output_max_length': 10 + 2,\n",
    "    'akara2017': True,\n",
    "    'test_step': 5\n",
    "}\n",
    "\n",
    "data_loader = SeqDataLoader(data_dir, num_folds, fold_idx, classes=classes)\n",
    "X_train, y_train, X_test, y_test = data_loader.load_data(seq_len=kargs['max_time_step'])\n",
    "print(X_train.shape)\n",
    "X_train = np.reshape(X_train,[X_train.shape[0]*X_train.shape[1],-1])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature\n",
      "torch.Size([46122, 3000])\n",
      "tensor([[-1.1789e-01, -5.1045e-01, -1.5799e-03,  ..., -5.9769e-01,\n",
      "         -5.1045e-01, -2.4875e-01],\n",
      "        [-1.4410e+00,  2.0158e-01,  1.0608e-01,  ..., -6.3881e-01,\n",
      "         -9.4441e-01, -6.1971e-01],\n",
      "        [-4.3664e-01, -6.5547e-01, -4.7120e-01,  ..., -1.0701e+00,\n",
      "         -1.0355e+00, -1.1622e+00],\n",
      "        ...,\n",
      "        [-2.1498e-01,  6.3238e-01, -2.7854e-01,  ..., -1.7932e+00,\n",
      "         -1.1683e+00, -1.1789e+00],\n",
      "        [-8.5821e-01, -1.1031e+00, -9.0718e-01,  ..., -1.1031e+00,\n",
      "         -4.9912e-01, -7.0315e-01],\n",
      "        [-2.2816e+00, -1.2748e+00, -3.1054e+00,  ...,  3.9107e-01,\n",
      "          9.4026e-01,  7.2058e-01]])\n",
      "label\n",
      "torch.Size([4612, 10])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [4, 4, 1, 1, 0, 1, 1, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 1, 2, 1, 1, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "        [2, 3, 3, 3, 2, 3, 3, 2, 2, 2],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X_train=torch.tensor(X_train[:X_train.shape[0]//4])\n",
    "y_train=torch.tensor(y_train[:y_train.shape[0]//4])\n",
    "print('feature')\n",
    "print(X_train.shape)\n",
    "print(X_train[:10])\n",
    "print('label')\n",
    "print(y_train.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path, channel_ename = os.path.split(data_dir)\n",
    "\n",
    "char2numY = dict(list(zip(classes, list(range(len(classes))))))        \n",
    "traindata_dir = os.path.join(os.path.abspath(os.path.join(data_dir, os.pardir)),'traindata/')\n",
    "\n",
    "char2numY['<SOD>'] = len(char2numY)\n",
    "char2numY['<EOD>'] = len(char2numY)\n",
    "num2charY = dict(list(zip(list(char2numY.values()), list(char2numY.keys()))))\n",
    "fname = os.path.join(traindata_dir,'trainData_'+channel_ename+'_SMOTE_all_10s_f'+str(fold_idx)+'.npz')\n",
    "\n",
    "nums = []\n",
    "for cl in classes:\n",
    "    nums.append(len(np.where(y_train == char2numY[cl])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57766\n",
      "{0: 61827, 1: 57766, 2: 64766, 3: 57766, 4: 57766}\n",
      "(299891, 3000)\n",
      "(299891,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "n_osamples = nums[2] - 7000\n",
    "ratio = {0: n_osamples if nums[0] < n_osamples else nums[0], 1: n_osamples if nums[1] < n_osamples else nums[1],\n",
    "            2: nums[2], 3: n_osamples if nums[3] < n_osamples else nums[3], 4: n_osamples if nums[4] < n_osamples else nums[4]}\n",
    "\n",
    "sm = SMOTE(random_state=12, sampling_strategy=ratio)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "print(n_osamples)\n",
    "print(ratio)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29989, 10, 3000)\n",
      "(29989, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train[:(X_train.shape[0] // kargs['max_time_step']) * kargs['max_time_step'], :]\n",
    "y_train = y_train[:(X_train.shape[0] // kargs['max_time_step']) * kargs['max_time_step']]\n",
    "\n",
    "X_train = np.reshape(X_train,[-1,X_test.shape[1],X_test.shape[2]])\n",
    "y_train = np.reshape(y_train,[-1,y_test.shape[1],])\n",
    "\n",
    "# shuffle training data_2013\n",
    "permute = np.random.permutation(len(y_train))\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = X_train[permute]\n",
    "y_train = y_train[permute]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29989, 12)\n",
      "('The training set after oversampling: ', ['W', 'N1', 'N2', 'N3', 'REM'])\n",
      "('W', 61827)\n",
      "('N1', 57766)\n",
      "('N2', 64766)\n",
      "('N3', 57766)\n",
      "('REM', 57765)\n"
     ]
    }
   ],
   "source": [
    "# add '<SOD>' to the beginning of each label sequence, and '<EOD>' to the end of each label sequence (both for training and test sets)\n",
    "y_train= [[char2numY['<SOD>']] + [y_ for y_ in date] + [char2numY['<EOD>']] for date in y_train]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "y_test= [[char2numY['<SOD>']] + [y_ for y_ in date] + [char2numY['<EOD>']] for date in y_test]\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(('The training set after oversampling: ', classes))\n",
    "for cl in classes:\n",
    "    print((cl, len(np.where(y_train==char2numY[cl])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': 0, 'N1': 1, 'N2': 2, 'N3': 3, 'REM': 4, '<SOD>': 5, '<EOD>': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2numY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(char2numY), embedding_dim=10)\n",
    "\n",
    "# Use the embedding layer\n",
    "decoder_emb_inputs = embedding_layer(torch.tensor(y_train[:, :-1]))\n",
    "\n",
    "# Get the embedding matrix (decoder_embedding)\n",
    "decoder_embedding = embedding_layer.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29989, 11)\n",
      "torch.Size([29989, 11, 10])\n",
      "torch.Size([7, 10])\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, :-1].shape)\n",
    "print(decoder_emb_inputs.shape)\n",
    "print(decoder_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention function f(⋅)\n",
    "attention probability α_i : 각각의 hidden state의 중요도에 관한 것\n",
    "scores와 hidden states를 곱해서 weighted combination c_t 를 구함\n",
    "\n",
    "f(h_(t-1),e_i )=tanh⁡(W_h h_(t-1)+W_e e_i )\n",
    "α_i=softmax(f(h_(t-1),e_i ))\n",
    "c_t=∑_(i=0)^n▒〖α_i e_i 〗\n",
    "α_i:hidden state의 part i에 대한 중요도\n",
    "\n",
    "즉, 매 time step t 마다, attiontion layer는 f(⋅)을 계산함:* encoder 의 hidden state e_i 와 decoder의 hidden state h_(t-1)의 조합에 tanh 씌운거\n",
    "\t그러고 나면, 이걸 softmax 에 넣어서, n개의 parts에 대해서 α_i를 계산함\n",
    "\t마지막으로, 모든 벡터 e_i 와 그에 해당하는 α_i의 attention module c_t를 계산함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_emb_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\sejin\\GoodSleep\\test3.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/sejin/GoodSleep/test3.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m decoder_emb_inputs\u001b[39m.\u001b[39mview(decoder_emb_inputs\u001b[39m.\u001b[39mshape(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder_emb_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_emb_inputs.view(decoder_emb_inputs.shape(0), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29989, 257])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((29989, 256))\n",
    "d = torch.ones((29989,1))+4\n",
    "decoder_embedding.view(29989, -1)\n",
    "torch.cat([a, d], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29989, 12)\n",
      "[[5 2 2 ... 2 2 6]\n",
      " [5 3 3 ... 3 3 6]\n",
      " [5 2 2 ... 2 2 6]\n",
      " ...\n",
      " [5 0 0 ... 0 0 6]\n",
      " [5 4 4 ... 4 4 6]\n",
      " [5 0 0 ... 0 0 6]]\n",
      "(29989, 11)\n",
      "[[5 2 2 ... 2 2 2]\n",
      " [5 3 3 ... 3 3 3]\n",
      " [5 2 2 ... 2 2 2]\n",
      " ...\n",
      " [5 0 0 ... 0 0 0]\n",
      " [5 4 4 ... 4 4 4]\n",
      " [5 0 0 ... 0 0 0]]\n",
      "(29989, 11)\n",
      "[[2 2 2 ... 2 2 6]\n",
      " [3 3 3 ... 3 3 6]\n",
      " [2 2 2 ... 2 2 6]\n",
      " ...\n",
      " [0 0 0 ... 0 0 6]\n",
      " [4 4 4 ... 4 4 6]\n",
      " [0 0 0 ... 0 0 6]]\n",
      "7\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "# decoder input\n",
    "print(y_train[:, :-1].shape)\n",
    "print(y_train[:, :-1])\n",
    "# label\n",
    "print(y_train[:, 1:].shape)\n",
    "print(y_train[:, 1:])\n",
    "\n",
    "# input_dim=len(char2numY), output_dim=hparams['embed_size']\n",
    "print(len(char2numY))\n",
    "print(kargs['embed_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29989, 7)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]], shape=(10, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 5\n",
    "import tensorflow as tf\n",
    "decoder_input = tf.one_hot(char2numY.get('<SOD>'), depth=NUM_CLASSES+2)\n",
    "decoder_input = tf.multiply(\n",
    "tf.ones((X_train.shape[0],NUM_CLASSES+2),dtype=tf.float32),\n",
    "decoder_input) # <SOD> 로 시작\n",
    "print(decoder_input.shape)\n",
    "print(decoder_input[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
