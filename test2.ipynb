{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data2013\n",
      "eeg_fpz_cz\n",
      "d:\\sejin\\GoodSleep\\data\\data2013\\traindata/\n",
      "2023-12-08 13:57:30.276860\n",
      "\n",
      "========== [Fold-0] ==========\n",
      "\n",
      "Load training set:\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4631E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4151E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4042E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4722E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4812G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4701E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4181E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4071E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4182E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4502E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4622E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4411E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4672G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4711E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4292G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4141E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4581G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4662E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4492G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4542F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4202E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4011E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4332F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4092E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4352F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4742E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4462F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4111E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4732E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4821G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4452F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4282G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4601E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4041E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4451F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4641E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4591G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4532E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4121E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4262F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4642E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4632E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4432E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4302E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4131E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4072E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4221E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4422E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4002E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4291G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4621E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4272F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4051E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4172E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4541F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4081E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4741E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4252E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4102E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4161E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4402E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4421E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4611E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4271F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4382F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4372F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4091E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4341F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4192E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4281G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4651E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4371F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4751E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4702E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4822G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4342F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4021E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4082E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4491G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4251E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4431E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4712E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4471F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4061E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4191E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4152E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4652E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4761E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4171E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4322E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4762E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4162E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4802G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4012E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4562F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4101E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4212E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4752E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4482F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4721E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4242E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4062E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4772G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4311E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4112E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4731E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4381F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4211E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4512E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4261F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4811G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4771G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4031E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4222E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4022E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4552F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4551F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4052E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4571F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4351F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4122E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4241E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4032E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4671G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4561F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4602E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4231E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4531E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4301E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4572F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4401E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4612E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4412E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4801G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4232E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4592G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4321E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4472F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4441E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4582G0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4481F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4362F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4201E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4661E0.npz ...\n",
      " \n",
      "Load Test set:\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4522E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4142E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4331F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4312E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4501E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4511E0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4461F0.npz ...\n",
      "Loading data/data2013/eeg_fpz_cz\\SC4442E0.npz ...\n",
      " \n",
      "Training set: n_subjects=144\n",
      "(1063, 3000)\n",
      "(952, 3000)\n",
      "(1200, 3000)\n",
      "(1130, 3000)\n",
      "(1183, 3000)\n",
      "(1717, 3000)\n",
      "(964, 3000)\n",
      "(976, 3000)\n",
      "(920, 3000)\n",
      "(1103, 3000)\n",
      "(1823, 3000)\n",
      "(1078, 3000)\n",
      "(1021, 3000)\n",
      "(1413, 3000)\n",
      "(1605, 3000)\n",
      "(1004, 3000)\n",
      "(1095, 3000)\n",
      "(1994, 3000)\n",
      "(1040, 3000)\n",
      "(1148, 3000)\n",
      "(1021, 3000)\n",
      "(1103, 3000)\n",
      "(1312, 3000)\n",
      "(1105, 3000)\n",
      "(963, 3000)\n",
      "(1063, 3000)\n",
      "(1022, 3000)\n",
      "(928, 3000)\n",
      "(2318, 3000)\n",
      "(1704, 3000)\n",
      "(1166, 3000)\n",
      "(1070, 3000)\n",
      "(1349, 3000)\n",
      "(1235, 3000)\n",
      "(1208, 3000)\n",
      "(1271, 3000)\n",
      "(1840, 3000)\n",
      "(1056, 3000)\n",
      "(1052, 3000)\n",
      "(980, 3000)\n",
      "(2049, 3000)\n",
      "(1107, 3000)\n",
      "(962, 3000)\n",
      "(854, 3000)\n",
      "(1028, 3000)\n",
      "(1273, 3000)\n",
      "(1099, 3000)\n",
      "(884, 3000)\n",
      "(1127, 3000)\n",
      "(1131, 3000)\n",
      "(1445, 3000)\n",
      "(1090, 3000)\n",
      "(672, 3000)\n",
      "(1773, 3000)\n",
      "(1716, 3000)\n",
      "(1134, 3000)\n",
      "(2210, 3000)\n",
      "(1020, 3000)\n",
      "(1092, 3000)\n",
      "(1144, 3000)\n",
      "(1072, 3000)\n",
      "(785, 3000)\n",
      "(1652, 3000)\n",
      "(1052, 3000)\n",
      "(1871, 3000)\n",
      "(1509, 3000)\n",
      "(1132, 3000)\n",
      "(1501, 3000)\n",
      "(1274, 3000)\n",
      "(1127, 3000)\n",
      "(2644, 3000)\n",
      "(918, 3000)\n",
      "(2044, 3000)\n",
      "(1515, 3000)\n",
      "(1366, 3000)\n",
      "(1582, 3000)\n",
      "(1025, 3000)\n",
      "(1054, 3000)\n",
      "(1101, 3000)\n",
      "(972, 3000)\n",
      "(699, 3000)\n",
      "(1241, 3000)\n",
      "(1187, 3000)\n",
      "(843, 3000)\n",
      "(1535, 3000)\n",
      "(1762, 3000)\n",
      "(1929, 3000)\n",
      "(1683, 3000)\n",
      "(1002, 3000)\n",
      "(1021, 3000)\n",
      "(2662, 3000)\n",
      "(1003, 3000)\n",
      "(1229, 3000)\n",
      "(1186, 3000)\n",
      "(1148, 3000)\n",
      "(1104, 3000)\n",
      "(808, 3000)\n",
      "(1049, 3000)\n",
      "(1910, 3000)\n",
      "(1031, 3000)\n",
      "(1775, 3000)\n",
      "(1016, 3000)\n",
      "(1324, 3000)\n",
      "(1054, 3000)\n",
      "(802, 3000)\n",
      "(2667, 3000)\n",
      "(1776, 3000)\n",
      "(1578, 3000)\n",
      "(954, 3000)\n",
      "(1597, 3000)\n",
      "(1293, 3000)\n",
      "(1325, 3000)\n",
      "(952, 3000)\n",
      "(1108, 3000)\n",
      "(1009, 3000)\n",
      "(1090, 3000)\n",
      "(1047, 3000)\n",
      "(1246, 3000)\n",
      "(1236, 3000)\n",
      "(976, 3000)\n",
      "(977, 3000)\n",
      "(1673, 3000)\n",
      "(911, 3000)\n",
      "(1968, 3000)\n",
      "(1237, 3000)\n",
      "(2043, 3000)\n",
      "(904, 3000)\n",
      "(1096, 3000)\n",
      "(929, 3000)\n",
      "(1095, 3000)\n",
      "(1064, 3000)\n",
      "(1062, 3000)\n",
      "(924, 3000)\n",
      "(1241, 3000)\n",
      "(1729, 3000)\n",
      "(1231, 3000)\n",
      "(1560, 3000)\n",
      "(2161, 3000)\n",
      "(1195, 3000)\n",
      "(1175, 3000)\n",
      "(2027, 3000)\n",
      "(824, 3000)\n",
      "(1022, 3000)\n",
      "(2026, 3000)\n",
      "Number of examples = 185132\n",
      "W: 62763\n",
      "N1: 20175\n",
      "N2: 65365\n",
      "N3: 12366\n",
      "REM: 24463\n",
      " \n",
      "Test set: n_subjects = 8\n",
      "(997, 3000)\n",
      "(952, 3000)\n",
      "(1888, 3000)\n",
      "(1181, 3000)\n",
      "(1326, 3000)\n",
      "(1087, 3000)\n",
      "(983, 3000)\n",
      "(1092, 3000)\n",
      "Number of examples = 9506\n",
      "W: 3000\n",
      "N1: 1289\n",
      "N2: 3517\n",
      "N3: 453\n",
      "REM: 1247\n",
      " \n",
      "1702011450.27686\n",
      "[[[ 1.4335219   0.85675293  0.31719497 ... -0.23166575 -0.3339957\n",
      "   -0.85494816]\n",
      "  [-1.257927   -0.9034113  -1.2485975  ...  1.0184362   1.4382571\n",
      "    1.5408801 ]\n",
      "  [ 1.3719528   1.1786772   1.0866412  ... -0.00858696  0.27672458\n",
      "    0.35035336]\n",
      "  ...\n",
      "  [-1.0473737  -1.1712427  -1.1712427  ...  0.14627443  0.4953602\n",
      "   -0.02263802]\n",
      "  [-0.05748278 -0.19938442 -0.19938442 ... -0.07640299 -0.29398552\n",
      "   -0.42642704]\n",
      "  [-0.61361736 -0.9360357  -0.5061446  ... -0.51591486 -0.6722389\n",
      "   -0.6917794 ]]\n",
      "\n",
      " [[ 0.27956143 -0.0331471   0.41671428 ... -0.63661975 -0.26905\n",
      "    0.14789468]\n",
      "  [-0.70323646 -0.40006414 -0.2606049  ...  1.2431297   0.6307216\n",
      "    0.5033893 ]\n",
      "  [ 0.253573    0.1893929   0.19741541 ...  0.24822466  0.26426968\n",
      "    0.64935046]\n",
      "  ...\n",
      "  [-1.1556306  -1.262896   -0.9095513  ... -1.0357457  -1.2881348\n",
      "   -1.654099  ]\n",
      "  [-1.3081341  -1.249542   -0.39670122 ... -2.1739955  -2.1088932\n",
      "   -2.0242603 ]\n",
      "  [-2.0017025  -1.3854929  -0.7285903  ...  0.15503097  0.6666011\n",
      "    0.20735066]]\n",
      "\n",
      " [[-0.94974077 -0.7103259  -0.91709334 ...  0.7479285   1.5314683\n",
      "    0.41057116]\n",
      "  [-0.29576528 -0.05456177  0.8757947  ... -0.6747994  -1.0538336\n",
      "   -0.6862853 ]\n",
      "  [-0.4623554   0.6772014   0.724683   ... -0.09437351 -0.43861464\n",
      "    0.03620071]\n",
      "  ...\n",
      "  [-0.9771965  -0.5732137  -0.14288406 ... -2.0925405  -1.4777839\n",
      "   -1.3899616 ]\n",
      "  [-0.86965704 -0.61591053  0.0655802  ... -0.02866852 -0.10841744\n",
      "   -0.43466297]\n",
      "  [-0.6872124  -0.76022977 -0.5898559  ... -0.07062132 -0.3708038\n",
      "   -0.14363867]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.5062704   1.7405595   1.5531281  ... -0.12438257 -0.36804333\n",
      "   -0.3258713 ]\n",
      "  [-0.7993379   0.38907793 -0.10771886 ... -1.0818301  -0.9259724\n",
      "   -1.8367665 ]\n",
      "  [-1.1575584  -1.2537518  -1.0875996  ... -0.08631456  0.05797543\n",
      "    0.03174088]\n",
      "  ...\n",
      "  [ 0.38495946  0.3795694   0.3957395  ... -1.38836    -0.8816974\n",
      "    0.4119096 ]\n",
      "  [-0.45655745  0.0971502   0.23953217 ...  1.3153071   1.6264381\n",
      "    1.1571048 ]\n",
      "  [ 0.29443112  0.22383843  0.8732912  ... -0.8350519  -0.70327884\n",
      "   -0.4726761 ]]\n",
      "\n",
      " [[-0.18927903 -1.62634    -1.6295335  ...  1.493679    1.704448\n",
      "    1.6469655 ]\n",
      "  [ 1.4465259   1.3384157   1.3547962  ...  0.3228355   0.35559615\n",
      "    0.04436994]\n",
      "  [-0.13093503 -0.10578148  0.18887444 ...  0.8572403   1.464519\n",
      "    1.651374  ]\n",
      "  ...\n",
      "  [-0.04704103 -0.0606099  -0.22004418 ...  0.5839117   0.41090852\n",
      "    0.10560882]\n",
      "  [-0.16016074 -0.03969805  0.2614587  ...  1.6300488   1.616664\n",
      "    1.6969725 ]\n",
      "  [ 1.8911577   1.5896037   1.4951409  ...  0.7176397   0.53598046\n",
      "    0.5505132 ]]\n",
      "\n",
      " [[ 0.5246222   0.4947622   0.42757732 ...  0.688852    0.47983226\n",
      "    0.14390762]\n",
      "  [ 0.26006928  0.5840451   0.704379   ...  1.4171258   0.9450467\n",
      "    0.3341209 ]\n",
      "  [ 0.14439435 -0.02178627 -0.17965788 ...  1.3990581   1.6898742\n",
      "    1.6566379 ]\n",
      "  ...\n",
      "  [ 1.3540523   1.4706966   1.3799733  ... -0.1104816  -0.47337496\n",
      "   -0.5252169 ]\n",
      "  [-0.6009424  -0.35131642 -0.6777503  ... -0.5913413  -0.43772537\n",
      "   -0.49533138]\n",
      "  [-0.9156279  -0.5599099  -0.84832996 ...  0.8052784   0.57454234\n",
      "    0.31496432]]]\n",
      "[[4 4 4 ... 4 4 4]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [4 4 4 ... 4 4 4]]\n",
      "[[[ 4.81491983e-01  1.13345265e+00  2.62162399e+00 ...  4.67318922e-01\n",
      "    9.35029864e-01  7.79126227e-01]\n",
      "  [ 1.19994916e-01  9.51470993e-03  7.92919695e-01 ...  2.96020173e-02\n",
      "    6.02090299e-01  1.30038574e-01]\n",
      "  [-5.33694446e-01 -7.65286267e-01  2.41247430e-01 ... -1.74509799e+00\n",
      "   -1.37098813e+00 -8.00915718e-01]\n",
      "  ...\n",
      "  [ 1.48052096e-01  2.75021285e-01  4.01990443e-01 ... -1.60301626e-01\n",
      "   -3.32616955e-01 -6.22832239e-01]\n",
      "  [-4.51961994e-01 -6.46419287e-01 -4.08749253e-01 ... -1.60790253e+00\n",
      "   -1.38103569e+00 -1.32701981e+00]\n",
      "  [-3.86966228e-01 -3.20594698e-01 -1.21480055e-01 ...  5.24134099e-01\n",
      "    5.34995012e-02  2.33306158e-02]]\n",
      "\n",
      " [[-2.89625049e-01  1.23512343e-01  9.97002780e-01 ...  5.26887774e-02\n",
      "   -1.47977948e-01  2.76963353e-01]\n",
      "  [ 3.58713627e-01  2.15685844e-01  6.07391186e-02 ...  9.64960605e-02\n",
      "    1.08415037e-01 -2.26937514e-02]\n",
      "  [ 1.34061709e-01  3.43416780e-01 -3.34223844e-02 ...  2.31760740e-01\n",
      "    3.43416780e-01 -3.26519519e-01]\n",
      "  ...\n",
      "  [ 6.40435636e-01  2.50001252e-01  4.14394706e-01 ... -3.25375676e-01\n",
      "   -7.36359179e-01 -6.64437115e-01]\n",
      "  [-1.25622034e+00 -3.85888547e-01 -2.14930505e-01 ...  7.17567801e-01\n",
      "    3.75651747e-01  6.86484516e-01]\n",
      "  [ 2.35358980e-02  3.79301727e-01 -3.67806554e-01 ... -5.94761446e-02\n",
      "    3.20007414e-01  1.65842220e-01]]\n",
      "\n",
      " [[ 2.82587111e-01  1.78676590e-01  1.48987874e-01 ...  1.53886611e-02\n",
      "    3.76551971e-02  3.76551971e-02]\n",
      "  [-1.79591060e-01 -1.97145835e-01  3.29497039e-01 ...  2.24168465e-01\n",
      "   -4.04345198e-03 -4.04345198e-03]\n",
      "  [-6.44748956e-02  4.40354258e-01  6.65916204e-01 ... -1.07439086e-01\n",
      "   -2.47072667e-01 -9.66980383e-02]\n",
      "  ...\n",
      "  [-1.08431649e+00 -1.28722084e+00 -1.28722084e+00 ... -3.26806813e-01\n",
      "    3.22487146e-01  5.11864603e-01]\n",
      "  [ 8.01968500e-02 -6.48582056e-02 -6.48582056e-02 ...  3.36832702e-01\n",
      "    9.13549289e-02  4.03781205e-01]\n",
      "  [ 3.96446496e-01  2.65408354e-03  5.21307468e-01 ... -1.22206919e-01\n",
      "   -7.41834491e-02 -1.79835081e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.15130448e+00 -1.89498425e+00 -1.25507390e+00 ... -6.15163326e-01\n",
      "   -2.11616263e-01 -3.73035073e-01]\n",
      "  [-1.17712736e+00 -1.76831853e+00 -9.48948324e-01 ...  1.18763733e+00\n",
      "   -4.92590159e-01  6.79420412e-01]\n",
      "  [-4.85441774e-01  4.12519336e-01 -8.94215167e-01 ...  4.66128975e-01\n",
      "   -3.58118951e-01  7.14073420e-01]\n",
      "  ...\n",
      "  [-1.36942077e+00 -1.13849783e+00 -1.25707984e+00 ... -6.26722753e-01\n",
      "    1.47181123e-01 -4.83176082e-01]\n",
      "  [-1.32915914e-01 -4.23390478e-01  1.18172280e-01 ... -5.42053747e+00\n",
      "   -1.65913832e+00 -7.30123758e+00]\n",
      "  [-4.40229207e-01 -6.55834079e-01  5.32238543e-01 ...  5.72664499e-01\n",
      "    9.47727084e-01  1.05328369e+00]]\n",
      "\n",
      " [[-4.94444668e-01  8.28350410e-02 -1.73733741e-01 ...  1.01161383e-01\n",
      "    4.49361861e-01  7.36718699e-02]\n",
      "  [ 3.15758586e-01 -3.60162437e-01  1.93871155e-01 ...  8.03308189e-01\n",
      "    6.14936769e-01  7.47904837e-01]\n",
      "  [ 6.02755666e-01  5.06326079e-01  6.31684542e-01 ... -3.90469164e-01\n",
      "   -9.01545882e-01 -7.27972627e-01]\n",
      "  ...\n",
      "  [ 9.95096385e-01  9.66296792e-01  1.50388920e+00 ... -5.79281390e-01\n",
      "    2.75106579e-01 -8.19278002e-01]\n",
      "  [ 1.99518785e-01 -2.17747569e-01  3.22244197e-01 ... -2.13869475e-02\n",
      "    5.59513330e-01 -5.02356049e-03]\n",
      "  [ 8.31867635e-01  1.64159715e-01  1.48844719e+00 ...  2.64315933e-01\n",
      "   -5.48062146e-01  2.64315933e-01]]\n",
      "\n",
      " [[ 4.06879596e-02  5.34873784e-01  6.19246960e-01 ... -4.53497887e-01\n",
      "    7.68479034e-02 -1.28058434e-01]\n",
      "  [ 6.47967517e-01  1.53662309e-01  6.05898976e-01 ...  1.01076655e-01\n",
      "   -7.40293920e-01  1.04761863e+00]\n",
      "  [-1.06445633e-01  9.96345818e-01  8.66605639e-01 ... -7.36612201e-01\n",
      "   -2.17651486e-01 -1.12583268e+00]\n",
      "  ...\n",
      "  [ 3.86194050e-01 -3.83176893e-01 -2.11199876e-02 ... -2.58719832e-01\n",
      "    6.91679478e-01 -4.17119712e-01]\n",
      "  [ 1.37888956e+00  7.12982476e-01  1.94581056e+00 ...  8.74959946e-01\n",
      "    1.23490977e+00  1.10892737e+00]\n",
      "  [ 1.29353333e+00  7.96586096e-01  1.26737809e+00 ...  3.25793952e-01\n",
      "    3.17075580e-01 -3.10647219e-01]]]\n",
      "[[1 1 1 ... 2 2 2]\n",
      " [1 1 2 ... 2 2 2]\n",
      " [2 2 1 ... 2 2 2]\n",
      " ...\n",
      " [2 1 2 ... 2 2 0]\n",
      " [1 1 1 ... 2 2 2]\n",
      " [4 4 4 ... 4 2 2]]\n",
      "{'W': 0, 'N1': 1, 'N2': 2, 'N3': 3, 'REM': 4}\n",
      "{0: 'W', 1: 'N1', 2: 'N2', 3: 'N3', 4: 'REM', 5: '<SOD>', 6: '<EOD>'}\n",
      "[62761, 20175, 65365, 12366, 24463]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 900831000 into shape (10,3000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m X_train[:(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m kargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_time_step\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m kargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_time_step\u001b[39m\u001b[38;5;124m'\u001b[39m], :]\n\u001b[0;32m     95\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train[:(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m kargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_time_step\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m kargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_time_step\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 97\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(y_train,[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],])\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 900831000 into shape (10,3000)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dataloader import SeqDataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_folds = 20\n",
    "data_dir = 'data/data2013/eeg_fpz_cz'\n",
    "classes = ['W', 'N1', 'N2', 'N3', 'REM']\n",
    "checkpoint_dir = 'checkpoints-seq2seq-sleep-EDF'\n",
    "output_dir = 'outputs_2013/outputs_eeg_fpz_cz'\n",
    "\n",
    "kargs = {\n",
    "    'epochs': 120,\n",
    "    'batch_size': 20,\n",
    "    'num_units': 128,\n",
    "    'embed_size': 10,\n",
    "    'input_depth': 3000,\n",
    "    'n_channels': 100,\n",
    "    'bidirectional': True,\n",
    "    'use_attention': True,\n",
    "    'lstm_layers': 2,\n",
    "    'attention_size': 64,\n",
    "    'beam_width': 4,\n",
    "    'use_beamsearch_decode': False,\n",
    "    'max_time_step': 10,\n",
    "    'output_max_length': 10 + 2,\n",
    "    'akara2017': True,\n",
    "    'test_step': 5\n",
    "}\n",
    "\n",
    "path, channel_ename = os.path.split(data_dir)\n",
    "traindata_dir = os.path.join(os.path.abspath(os.path.join(data_dir, os.pardir)),'traindata/')\n",
    "\n",
    "print(path)\n",
    "print(channel_ename)\n",
    "print(traindata_dir)\n",
    "\n",
    "print((str(datetime.now())))\n",
    "\n",
    "fold_idx = 0\n",
    "\n",
    "start_time_fold_i = time.time()\n",
    "data_loader = SeqDataLoader(data_dir, num_folds, fold_idx, classes=classes)\n",
    "X_train, y_train, X_test, y_test = data_loader.load_data(seq_len=kargs['max_time_step'])\n",
    "\n",
    "print(start_time_fold_i)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "# preprocessing\n",
    "char2numY = dict(list(zip(classes, list(range(len(classes))))))        \n",
    "pre_f1_macro = 0\n",
    "\n",
    "print(char2numY)\n",
    "\n",
    "# <SOD> is a token to show start of decoding  and <EOD> is a token to indicate end of decoding\n",
    "char2numY['<SOD>'] = len(char2numY)\n",
    "char2numY['<EOD>'] = len(char2numY)\n",
    "num2charY = dict(list(zip(list(char2numY.values()), list(char2numY.keys()))))\n",
    "\n",
    "print(num2charY)\n",
    "\n",
    "nums = []\n",
    "for cl in classes:\n",
    "    nums.append(len(np.where(y_train == char2numY[cl])[0]))\n",
    "\n",
    "print(nums)\n",
    "if (os.path.exists(traindata_dir) == False):\n",
    "    os.mkdir(traindata_dir)\n",
    "fname = os.path.join(traindata_dir,'trainData_'+channel_ename+'_SMOTE_all_10s_f'+str(fold_idx)+'.npz')\n",
    "\n",
    "if (os.path.isfile(fname)):\n",
    "    X_train, y_train,_ = data_loader.load_npz_file(fname)\n",
    "else:\n",
    "    # oversampling\n",
    "    n_osamples = nums[2] - 7000\n",
    "    ratio = {0: n_osamples if nums[0] < n_osamples else nums[0], 1: n_osamples if nums[1] < n_osamples else nums[1],\n",
    "                2: nums[2], 3: n_osamples if nums[3] < n_osamples else nums[3], 4: n_osamples if nums[4] < n_osamples else nums[4]}\n",
    "\n",
    "    sm = SMOTE(random_state=12, sampling_strategy=ratio)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    data_loader.save_to_npz_file(X_train, y_train,data_loader.sampling_rate,fname)\n",
    "\n",
    "    print(n_osamples)\n",
    "    print(ratio)\n",
    "\n",
    "    X_train = X_train[:(X_train.shape[0] // kargs['max_time_step']) * kargs['max_time_step'], :]\n",
    "y_train = y_train[:(X_train.shape[0] // kargs['max_time_step']) * kargs['max_time_step']]\n",
    "\n",
    "X_train = np.reshape(X_train,[-1,X_test.shape[1],X_test.shape[2]])\n",
    "y_train = np.reshape(y_train,[-1,y_test.shape[1],])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# shuffle training data_2013\n",
    "permute = np.random.permutation(len(y_train))\n",
    "X_train = X_train[permute]\n",
    "y_train = y_train[permute]\n",
    "\n",
    "# # add '<SOD>' to the beginning of each label sequence, and '<EOD>' to the end of each label sequence (both for training and test sets)\n",
    "# tar_train= [[char2numY['<SOD>']] + [y_ for y_ in date] + [char2numY['<EOD>']] for date in y_train]\n",
    "# tar_train = np.array(tar_train)\n",
    "\n",
    "# tar_test= [[char2numY['<SOD>']] + [y_ for y_ in date] + [char2numY['<EOD>']] for date in y_test]\n",
    "# tar_test = np.array(tar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7547, 10, 3000])\n",
      "torch.Size([7547, 10])\n"
     ]
    }
   ],
   "source": [
    "X_train=torch.tensor(X_train[:X_train.shape[0]//4])\n",
    "y_train=torch.tensor(y_train[:y_train.shape[0]//4])\n",
    "print(X_train.shape)\n",
    "print(X_train[:10])\n",
    "print(y_train.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7547, 3000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:294: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ..\\aten\\src\\ATen\\native\\Convolution.cpp:660.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7547, 128, 1])\n",
      "torch.Size([7547, 128, 1])\n"
     ]
    }
   ],
   "source": [
    "cnn1 = nn.Sequential(\n",
    "    nn.Conv1d(3000, 64, 50, stride=6, padding=25),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(8, stride=8, padding=4),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Conv1d(64, 128, 8, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(128, 128, 8, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(128, 128, 8, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(4, stride=4, padding=2),\n",
    ")\n",
    "\n",
    "cnn2 = nn.Sequential(\n",
    "    nn.Conv1d(3000, 64, 400, stride=50, padding=200),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(4, stride=4, padding=2),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Conv1d(64, 128, 6, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(128, 128, 6, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(128, 128, 6, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2, padding=1),\n",
    ")\n",
    "\n",
    "X_train = X_train.unsqueeze(-1)\n",
    "new_x = X_train[:,0]\n",
    "print(new_x.shape)\n",
    "\n",
    "out1 = cnn1(new_x)\n",
    "print(out1.shape)\n",
    "out2 = cnn2(new_x)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7547, 128])\n",
      "torch.Size([7547, 128])\n"
     ]
    }
   ],
   "source": [
    "# Flatten\n",
    "flatten_out1 = torch.flatten(out1, start_dim=1)\n",
    "print(flatten_out1.shape)\n",
    "\n",
    "# Flatten again\n",
    "flatten_out2 = torch.flatten(out2, start_dim=1)\n",
    "print(flatten_out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7547, 256])\n"
     ]
    }
   ],
   "source": [
    "# Concat\n",
    "flatten_out = torch.cat((flatten_out1, flatten_out2), dim=-1)\n",
    "flatten_out = F.dropout(flatten_out, 0.5)\n",
    "print(flatten_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3000, 64, 50, stride=6, padding=25),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(8, stride=8, padding=4),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv1d(64, 128, 8, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 8, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 8, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4, stride=4, padding=2),\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(3000, 64, 400, stride=50, padding=200),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4, stride=4, padding=2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv1d(64, 128, 6, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 6, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 6, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.cnn1(x)\n",
    "        out2 = self.cnn2(x)\n",
    "\n",
    "        # Flatten\n",
    "        flatten_out1 = torch.flatten(out1, start_dim=1)\n",
    "        flatten_out2 = torch.flatten(out2, start_dim=1)\n",
    "\n",
    "        # Concat\n",
    "        flatten_out = torch.cat((flatten_out1, flatten_out2), dim=-1)\n",
    "        flatten_out = F.dropout(flatten_out, 0.5)\n",
    "        return flatten_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7547, 3000, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(**kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    _x = cnn(X_train[:, i])\n",
    "    _x = _x.unsqueeze(1)\n",
    "    x.append(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7547, 10, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat(x, dim = 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7547, 10, 256])\n",
      "torch.Size([7547, 256])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(2 * kargs['num_units'], kargs['num_units'], num_layers = kargs['lstm_layers'], bidirectional = True, batch_first = True)\n",
    "\n",
    "encoder_outputs, (encoder_state, _) = lstm(x)\n",
    "# 마지막 레이어의 양방향 hidden state 가져오기\n",
    "hidden_state = encoder_state[-2:, :, :]\n",
    "\n",
    "# 양방향 hidden state 연결\n",
    "hidden_state = hidden_state.transpose(0, 1).contiguous().view(x.size(0), -1)\n",
    "print(encoder_outputs.shape)\n",
    "print(hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7547, 10, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7547, 10, 256])\n",
      "torch.Size([7547, 256])\n"
     ]
    }
   ],
   "source": [
    "lstm_f = nn.LSTM(2 * kargs['num_units'], kargs['num_units'], batch_first = True)\n",
    "lstm_b = nn.LSTM(2 * kargs['num_units'], kargs['num_units'], batch_first = True)\n",
    "\n",
    "\n",
    "f, (fh, _) = lstm_f(x)\n",
    "x_reversed = torch.flip(x, [1])\n",
    "b, (bh, _) = lstm_b(x_reversed)\n",
    "fh = fh.squeeze(0)\n",
    "bh = bh.squeeze(0)\n",
    "encoder_output = torch.cat([f, b], dim=-1)\n",
    "h = torch.cat([fh, bh], dim=-1)\n",
    "print(encoder_output.shape)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 7\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        self.We = nn.Linear(256, 256)\n",
    "        self.Wh = nn.Linear(256, 256)\n",
    "        self.tanh=torch.tanh\n",
    "        self.softmax = torch.nn.functional.softmax\n",
    "\n",
    "    # encoder_hidden과 decoder_hidden과 shape이 같아야 함\n",
    "    def forward(self, encoder_hidden, decoder_hidden):\n",
    "        WE = self.We(encoder_hidden)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
    "        WH = self.Wh(decoder_hidden)\n",
    "        x = WE + WH\n",
    "        f = self.tanh(x)\n",
    "        alpha = self.softmax(f, dim=-1)\n",
    "        c = alpha * encoder_hidden\n",
    "        c = torch.sum(c, dim=1)\n",
    "        return c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = Attention(**kargs)\n",
    "        self.lstm = nn.LSTM(263, kargs['num_units'], num_layers = kargs['lstm_layers'], bidirectional = True, batch_first = True)\n",
    "        self.classes = nn.Linear(2 * kargs['num_units'], NUM_CLASS)\n",
    "        # self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, decoder_input, prev_decoder_hidden, encoder_hidden):\n",
    "        c = self.attention(encoder_hidden, prev_decoder_hidden)\n",
    "        x = torch.cat([c, decoder_input], dim=-1)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, h = self.lstm(x) # .squeeze(1)\n",
    "        prediction = self.classes(x)\n",
    "        return prediction, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7547, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['W', 'N1', 'N2', 'N3', 'REM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 2,  ..., 3, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 0, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [4, 4, 4,  ..., 4, 4, 4],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 1, 0],\n",
      "         [0, 0, 1, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0, 1, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 1, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 1, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         ...,\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "# # convert to long tensor\n",
    "# y_train = y_train.long()\n",
    "\n",
    "# # one-hot encoding for each row\n",
    "# one_hot_y_train = torch.stack([F.one_hot(row, num_classes=5) for row in y_train])\n",
    "\n",
    "# print(one_hot_y_train)\n",
    "# one_hot_y_train.shape # torch.Size([7547, 10, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 inference할 때는 feature = [1, 10, 3000, 1]으로 하면 될듯\n",
    "\n",
    "# one hot encodding을 사용할 때는 output layer의 output_size=label\n",
    "# 결과 shape (batch_size, seq_len, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(**kargs)\n",
    "decoder_input = F.one_hot(torch.tensor(char2numY['<SOD>']), num_classes=7).float()\n",
    "decoder_input = decoder_input.expand(X_train.shape[0], -1)  # <SOD>로 시작\n",
    "pred, h = decoder(decoder_input, hidden_state, encoder_outputs)\n",
    "pred = pred.squeeze(1)  # Remove the second dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7547, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7547, 7])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7547, 10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9337, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9284, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\sejin\\GoodSleep\\test2.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sejin/GoodSleep/test2.ipynb#X62sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m _loss \u001b[39m=\u001b[39m cross_entropy(pred, y_train[:, t]\u001b[39m.\u001b[39mlong())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sejin/GoodSleep/test2.ipynb#X62sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(_loss)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/sejin/GoodSleep/test2.ipynb#X62sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m _loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sejin/GoodSleep/test2.ipynb#X62sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "decoder_input = F.one_hot(torch.tensor(char2numY['<SOD>']), num_classes=7).float()\n",
    "decoder_input = decoder_input.expand(X_train.shape[0], -1)  # <SOD>로 시작\n",
    "# print(decoder_input.shape)\n",
    "decoder = Decoder(**kargs)\n",
    "optimizer = optim.Adam(decoder.parameters())  # Define an optimizer\n",
    "\n",
    "pred, h = decoder(decoder_input, hidden_state, encoder_outputs)\n",
    "_loss = 0\n",
    "for t in range(10):\n",
    "    pred, h = decoder(decoder_input, hidden_state, encoder_outputs)\n",
    "    pred = pred.squeeze(1)  # Remove the second dimension\n",
    "    _loss = cross_entropy(pred, y_train[:, t].long())\n",
    "    print(_loss)\n",
    "    _loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "min_loss = 1e10\n",
    "min_epoch = 0\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i,x in enumerate(train_seq_dataset):\n",
    "        train_step(x)\n",
    "        \n",
    "    for i,x in enumerate(valid_seq_dataset):\n",
    "        valid_step(x)\n",
    "    \n",
    "    ipd.clear_output(wait=True)\n",
    "    print(f\"{e+1}/{EPOCHS}, loss={loss.result():.8f}, train acc={acc.result()*100:.2f}%,\")\n",
    "    print(f\"validation: loss={valid_loss.result():.8f}, acc={valid_acc.result()*100:.2f}%, {time.time()-start_time:.2f} sec/epoch, totally {time.time()-total_time:.2f} seconds\")\n",
    "    print(f\"\\tbest valid loss = {min_loss:.8f} at epoch-{min_epoch}\")\n",
    "    \n",
    "    if min_loss > valid_loss.result():\n",
    "        min_loss = valid_loss.result()\n",
    "        min_epoch = e\n",
    "        # BEST MODEL만 저장\n",
    "#         encoder.save_weights(f\"../weights/sleepeegnet/encoder-{SEED}\")\n",
    "#         decoder.save_weights(f\"../weights/sleepeegnet/decoder-{SEED}\")\n",
    "    \n",
    "    loss.reset_states()\n",
    "    acc.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    valid_acc.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
